[
  {
    "objectID": "00.Preparation.html#there-can-be-only-one-python-environment",
    "href": "00.Preparation.html#there-can-be-only-one-python-environment",
    "title": "00 Preparation",
    "section": "1 There can be only one (Python environment)",
    "text": "1 There can be only one (Python environment)\nActually that is not true, there are and can be many Python environments. There are so many options for using Python that it becomes a burden of choice for new users, and most regular users will choose different environments to fit different simple, common tasks.\nWe might quickly make a comparison with using the R programming language. For typical R use we can just “install and go” and we do not need to think about computer software and hardware “system administration”. When using R most people:\n\nUse the newest version of R possible\nUse the same editor, probably RStudio\nObtain packages from the exact same repository, CRAN (the Comprehensive R Archive Network)\nR package version management is largely unnecessary\nWrite and run R code on our local computer with no setup or configuration required\n\n\nPython is different, because system adminsitration in some form is often required, even for basic use. In Python:\n\nWe must pay attention to our Python version (major version 2, major version 3, version 3.7 or higher, etc.)\nThere are MANY editors, like IDLE, Spyder, RStudio, Jupyter notebooks, and so many more\nPython libraries (exactly like R packages) come from several major repositories and version incompatibility is ahem challenging\nWe must actively manage library versions (this requires moderate to great practice)\nWe often write and run code locally, on edge devices, or in the cloud depending on our need or application\nMany cloud options for running Python exist, each with pros and cons (Google Colab, Kaggle, Amazon Sagemaker Studio Lab, many others); of course more advanced management options exist too"
  },
  {
    "objectID": "00.Preparation.html#anaconda-and-google-colab",
    "href": "00.Preparation.html#anaconda-and-google-colab",
    "title": "00 Preparation",
    "section": "2 Anaconda and Google Colab",
    "text": "2 Anaconda and Google Colab\nWe will discuss and use two different alternatives to managing our python installation and our Python libraries. Here we will briefly mention each.\n\n\n2.1 Anaconda\nAnaconda is software designed to manage different environments for Python (and other software), libraries and editors. It is popular because it is easy to use and is free and open source. Part of the Anaconda ecosystem is the Python repository conda.\nYou may wish to watch the following video:\nAnaconda explanation on Youtube\n\nOtherwise you can download and install Anaconda:\nAnaconda distribution\n\nOnce installed you can create a new environment:\n\nA. Click the Environments tab\nB. Click Create +\nC. Name your new environment something sensible\nD. I recommend starting with Python 3.7 (the same version as Google Colab as of 2022-10-15)\n\nOnce you have created an environment, you can install Jupyter Lab:\n\nA. Click the Home tab\nB. Ensure your new environment is selected\nC. Find the Jupyter Lab panel\nD. Hit the install button\n\n\n\n2.2 Use of Jupyter lab\nUsing code embedded interactively with formatted text and images is very handy - like a webpage that allow you to enter and run computer code. This is the basic way that is popular to write and execute Python collaboratively. The tools we will highlight are are based around documents called “notebooks”.\n\nJupyter Lab iPython notebooks (.ipynb files)\nGoogle Colab iPython notebooks (also .ipynb files))\n\n\nThe basic idea is to edity code in “cells” and execute the code within a computing environment. The use of this system is very simple and powerful, but may seem counterintuitive at first. The following video may help illustrate the idea before we dive in and try Jupyter lab ourselves.\nJupyter notebook video\n\n\n\n2.3 Google Colab\nUsing Colab requires a Google account, so create one if you do not already have one. If you do already have one, you will probably want to have some free storage space on your Google Drive space (you have ~15GB for “free”). If you do not have enough space for managing some new data to your Google Drive, you might consider making a new google account just for the purpose of using Colab.\nYou may wish to watch the following video:\nColab explanation on Youtube\nYou can follow along with the video once you are signed into your Google account using your own Colab space"
  },
  {
    "objectID": "01.Anaconda.html#the-anaconda-and-python-labs",
    "href": "01.Anaconda.html#the-anaconda-and-python-labs",
    "title": "01 Anaconda",
    "section": "1 The Anaconda and Python labs",
    "text": "1 The Anaconda and Python labs\nThe purpose of this page is to get you up and running in the “ecosystem” of Anaconda and Jupyter Lab, and associated aspects of the workflow on your local machine.\nThe labs you will engage with are an introduction to Python and Pandas (a popular package for data manipulation and basic analysis in Python), or an easy refresh if you have already used the tools.\nAnaconda is widely used to manage toolsets and packages for data science projects including versions, environments and associated settings.\nJupyter Lab is an interface that is very popular for embedding formatted information with Python computer code and allows interactive computing right in a regular web browser.\n\n1.1 Jupyter Lab kernal\nLaunch Anaconda and Jupyter Lab\n\n\n\n\nAnaconda dash with Jupyter Lab - note the environment selection\n\n\n\n\nThe kernal for your session refers to the software engine that will execute your code. For some applications, you may want to manage several different kernals. For now do not worry about this and select the default kernal.\n\n\n\n\nChoose defaul kernal\n\n\n\n\n\n\n1.2 Working directory\nI recommend just using the navigation pane on the left hand side of Jupyter Lab to make a new folder that will be the working directory you will work with locally for these labs.\n\n\n\n\nCreate a working directory\n\n\n\n\nMy working directory looks like this:\n\n\n\n\nMy working directory"
  },
  {
    "objectID": "01.Anaconda.html#we-need-to-talk-about-the-console",
    "href": "01.Anaconda.html#we-need-to-talk-about-the-console",
    "title": "01 Anaconda",
    "section": "2 We need to talk about The Console",
    "text": "2 We need to talk about The Console\nThe Console implemented in various operating systems is powerful, fast way to manipulate files and to issue commands. You can accomplish a lot with very little knowledge in relative safety in modern computing environments. We will just scratch the surface here, but it is highly useful to get used to using the console.\n\n2.1 Launch a new console\n\n\n\n\nConsole launch\n\n\n\n\nMine looks like this (yours may differ slightly).\n\n\n\n\nConsole launch\n\n\n\n\nNote the console is a way to interact with our Python environment for “system commands” we may not wish to embed in a notebook."
  },
  {
    "objectID": "01.Anaconda.html#git-clone-the-lab-materials",
    "href": "01.Anaconda.html#git-clone-the-lab-materials",
    "title": "01 Anaconda",
    "section": "3 git clone the lab materials",
    "text": "3 git clone the lab materials\nThe Python lab materials are available on Github at https://github.com/C7082-2022/labs-python\nWe will use the Git command clone to make a copy of the labs-python repository on your local machine. Of course you can download all the stuff manually (like any muggle would), but it is far more powerful and so easy to clone it using Git.\n\n3.1 Using clone\nExecute the following command in your Console (remember shift + enter to execute a cell):\n!git clone https://github.com/C7082-2022/labs-python\n\nYour console should look similar to mine:\n\n\n\n\n!git clone\n\n\n\n\n\n\n3.2 Check the repo has been downloaded successfully\nYou can now browse the repo in your local file structure on the left file browser panel. Mine looks like this:\n\n\n\n\nYour local repo"
  },
  {
    "objectID": "01.Anaconda.html#open-first-lab",
    "href": "01.Anaconda.html#open-first-lab",
    "title": "01 Anaconda",
    "section": "4 Open first lab",
    "text": "4 Open first lab\nNavigate to the 01-Python folder inside the root labs-python folder.\nNow you can open the first lab notebook (just doublke click to open in Jupyter Lab) and work through the material interactively.\n\n\n\n\nThe notebooks"
  },
  {
    "objectID": "01.Anaconda.html#tips",
    "href": "01.Anaconda.html#tips",
    "title": "01 Anaconda",
    "section": "5 Tips",
    "text": "5 Tips\n\nStart a cheatsheet (e.g. in your own R markdown doc…) for Python (and Terminal) commands as a brief reference for yourself.\nRun all the commands yourself and experiment with your own code in new cells. Do not rush through these notebooks - invest in them.\nConsider taking time to learn and practice hotkeys in Jupyter Lab - this will be a valuable use of your time!\nWhen you open a new notebook, you can clear the outputs to avoid confusion when you run your own cells (the system I use for these pages prevents me from being able to do this)\n\n\n\n\n\nClear outputs"
  },
  {
    "objectID": "02.Colab.html#the-google-colab-labs",
    "href": "02.Colab.html#the-google-colab-labs",
    "title": "02 Colab",
    "section": "1 The Google Colab labs",
    "text": "1 The Google Colab labs\nThe purpose of these labs is to:\n\nIntroduce and practice the Google Colab environment\nPractice building neural networks in conventional and computer vision problems"
  },
  {
    "objectID": "02.Colab.html#colab",
    "href": "02.Colab.html#colab",
    "title": "02 Colab",
    "section": "2 Colab",
    "text": "2 Colab\nThe nice thing about Colab is that it is very easy to use, and provides free access to a virtual computing space through your web browser without the need to install anything locally. It is also a very powerful tool with a lot of options, possible workflows and applications. We will explore a little bit of this functionality while we also explore neural networks and computer vision in these labs.\nWe recommend using Google Chrome for these labs. Other browsers may work but may require a bit of extra adjustment.\n\nPros:\n\nfree\nno setup\nGPU (graphical processing unit) computing access\nAutomatically links to free Google account\n\ncons:\n\nRestricted administration rights\nTime limits on use\nWorkspace is not persistent\nRequires Google account\n\n\nYou will need to be signed in to your Google account, then you can launch Colab."
  },
  {
    "objectID": "02.Colab.html#lab-start",
    "href": "02.Colab.html#lab-start",
    "title": "02 Colab",
    "section": "3 Lab start",
    "text": "3 Lab start\nHere you will use Git to make a local copy of the notebooks you will use for this set of labs.\n\n3.1 start a new Colab notebook\nThe splash screen when you land on the Google Colab interface should look similar to the picture below. Begin by selecting the Github tab.\n\n\n\n\nMake a new notebook)\n\n\n\n\nYour notebook should look similar to mine. Enter the Github address location for our labs and hit the search (magnifying glass).\nhttps://github.com/C7082-2022/labs-cv-colab\n\n\n\n\nIdentify repo\n\n\n\n\nFind the first notebook, and click the icon to open it in a new browser tab\n\n\n\n\nLaunch notebook\n\n\n\n\nYou should see something similar to the picture below.\n\n\n\n\nFirst page\n\n\n\n\n\n\n3.2 Git clone the Colab lab notebooks\nYou will download the data and other files needed in the Github repo into your temporary Google Colab space. The environment is not persistent when you use Colab, so you will need to set up your environment every time you use it, for example on different days or between sessions. Luckily this is easy and is just a basic part of the Colab workflow.\nClick on the files icon on the left pane of your Colab notebook.\n\n\n\n\nFiles pane\n\n\n\n\nThis opens a visualization of your default workspace, where data and other assets may be stored in your cloud space,\n\n\n\n\nFiles pane\n\n\n\n\n\n\n3.3 Uncomment and run the first cells\nA first step for this first notebook will be to clear all the cell outputs. These are outputs created when the notebooks were first comiled (and the workflow for these pages prevents easily clearing them before you download them).\nFrom the dropdown menu: Edit > Clear all outputs\n\n\n\n\nClear outputs\n\n\n\n\nUncomment the second line of the the first two cells and run them one at a time. The first one ‘Clones’ the lab material for the notebooks into your workspace. NB the first time you run a notebook you did not author, you will get a warning. Here it is perfectly safe to ignore the warning (you know who the author is, the page is very simple and you can examine it before running, you know the purpose of the notebook, etc.).\nIt is prudent to heed the warning for random Colab notebooks you may find on the internet…\n\n\n\n\nUncomment\n\n\n\n\nAfter cloning, your workspace should look similar to mine (you may need to hit the refresh button for your folder visualization)\n\n\n\n\nCloned\n\n\n\n\nCongratulations - now you are ready to complete the Colab labs using the methods we have illustrated here.\n\n\n\n\nCraiyon.com: Winner"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "mysite",
    "section": "",
    "text": "Hello world"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to C7082. This module is a survey of methods in computational AI including aspects of neural networks and computer vision using Keras, Tensorflow and related systems. There is an emphasis on practical applications using a series of data examples and lab exercises, along with lectures on selected topics and readings. Prerequisites are familiarity with a statistical programming language (like Python or R) and familiarity with material in the R Stats Bootcamp."
  },
  {
    "objectID": "index.html#c7082-resources",
    "href": "index.html#c7082-resources",
    "title": "Home",
    "section": "C7082 Resources",
    "text": "C7082 Resources\n\nChollet 2022 Deep Learning with Python 2ed\nGoodfellow et al. 2017"
  },
  {
    "objectID": "index.html#harper-adams-data-science",
    "href": "index.html#harper-adams-data-science",
    "title": "Home",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "ipy-notebooks/00.Preparation.html#there-can-be-only-one-python-environment",
    "href": "ipy-notebooks/00.Preparation.html#there-can-be-only-one-python-environment",
    "title": "R Stats Bootcamp",
    "section": "1 There can be only one (Python environment)",
    "text": "1 There can be only one (Python environment)\nActually that is not true, there are and can be many Python environments. There are so many options for using Python that it becomes a burden of choice for new users, and most regular users will choose different environments to fit different simple, common tasks.\nWe might quickly make a comparison with using the R programming language. For typical R use we can just “install and go” and we do not need to think about computer software and hardware “system administration”. When using R most people:\n\nUse the newest version of R possible\nUse the same editor, probably RStudio\nObtain packages from the exact same repository, CRAN (the Comprehensive R Archive Network)\nR package version management is largely unnecessary\nWrite and run R code on our local computer with no setup or configuration required\n\n\nPython is different, because system adminsitration in some form is often required, even for basic use. In Python:\n\nWe must pay attention to our Python version (major version 2, major version 3, version 3.7 or higher, etc.)\nThere are MANY editors, like IDLE, Spyder, RStudio, Jupyter notebooks, and so many more\nPython libraries (exactly like R packages) come from several major repositories and version intercompaibility is ahem challenging\nWe must actively manage library versions (this requires moderate to great practice)\nWe often write and run code locally, on edge devices, or in the cloud depending on our need or application\nMany cloud options for running Python exist, each with pros and cons (Google Colab, Kaggle, Amazon Sagemaker Studio Lab, many others); of course more advanced management options exist too"
  },
  {
    "objectID": "ipy-notebooks/00.Preparation.html#anaconda-and-google-colab",
    "href": "ipy-notebooks/00.Preparation.html#anaconda-and-google-colab",
    "title": "R Stats Bootcamp",
    "section": "2 Anaconda and Google Colab",
    "text": "2 Anaconda and Google Colab\nWe will discuss and use two different alternatives to managing our python installation and our Python libraries. Here we will briefly mention each.\n\n\n2.1 Anaconda\nAnaconda is software designed to manage different environments for Python (and other software), libraries and editors. It is popular because it is easy to use and is free and open source. Part of the Anaconda ecosystem is the Python repository conda.\nYou may wish to watch the following video:\nAnaconda explanation on Youtube\n\nOtherwise you can download and install Anaconda:\nAnaconda distribution\n\nOnce installed you can create a new environment:\n\nA. Click the Environments tab\nB. Click Create +\nC. Name your new environment something sensible\nD. I recommend starting with Python 3.7 (the same version as Google Colab as of 2022-10-15)\n\nOnce you have created an environment, you can install Jupyter Lab:\n\nA. Click the Home tab\nB. Ensure your new environment is selected\nC. Find the Jupyter Lab panel\nD. Hit the install button\n\n\n\n2.2 Google Colab\nUsing colab requires a Google account, so create one if you do not already have one. If you do already have one, you will probably want to have some free storage space on your Google Drive space (you have ~15GB for “free”). If you do not have enough space for managing some new data to your Google Drive, you might consider making a new google account just for the purpose of using Colab.\nYou may wish to watch the following video:\nAnaconda explanation on Youtube\nYou can follow along with the video once you are signed into your Google account using your own Colab space"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html",
    "href": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html",
    "title": "R Stats Bootcamp",
    "section": "",
    "text": "This is a companion notebook for the book Deep Learning with Python, Second Edition. For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\nIf you want to be able to follow what’s going on, I recommend reading the notebook side by side with your copy of the book.\nThis notebook was generated for TensorFlow 2.6."
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#whats-tensorflow",
    "href": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#whats-tensorflow",
    "title": "R Stats Bootcamp",
    "section": "What’s TensorFlow?",
    "text": "What’s TensorFlow?"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#whats-keras",
    "href": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#whats-keras",
    "title": "R Stats Bootcamp",
    "section": "What’s Keras?",
    "text": "What’s Keras?"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#keras-and-tensorflow-a-brief-history",
    "href": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#keras-and-tensorflow-a-brief-history",
    "title": "R Stats Bootcamp",
    "section": "Keras and TensorFlow: A brief history",
    "text": "Keras and TensorFlow: A brief history"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#setting-up-a-deep-learning-workspace",
    "href": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#setting-up-a-deep-learning-workspace",
    "title": "R Stats Bootcamp",
    "section": "Setting up a deep-learning workspace",
    "text": "Setting up a deep-learning workspace\n\nJupyter notebooks: The preferred way to run deep-learning experiments\n\n\nUsing Colaboratory\n\nFirst steps with Colaboratory\n\n\nInstalling packages with pip\n\n\nUsing the GPU runtime"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#first-steps-with-tensorflow",
    "href": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#first-steps-with-tensorflow",
    "title": "R Stats Bootcamp",
    "section": "First steps with TensorFlow",
    "text": "First steps with TensorFlow\n\nConstant tensors and variables\nAll-ones or all-zeros tensors\n\nimport tensorflow as tf\nx = tf.ones(shape=(2, 1))\nprint(x)\n\n\nx = tf.zeros(shape=(2, 1))\nprint(x)\n\nRandom tensors\n\nx = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)\nprint(x)\n\n\nx = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.)\nprint(x)\n\nNumPy arrays are assignable\n\nimport numpy as np\nx = np.ones(shape=(2, 2))\nx[0, 0] = 0.\n\nCreating a TensorFlow variable\n\nv = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))\nprint(v)\n\nAssigning a value to a TensorFlow variable\n\nv.assign(tf.ones((3, 1)))\n\nAssigning a value to a subset of a TensorFlow variable\n\nv[0, 0].assign(3.)\n\nUsing assign_add\n\nv.assign_add(tf.ones((3, 1)))\n\n\n\nTensor operations: Doing math in TensorFlow\nA few basic math operations\n\na = tf.ones((2, 2))\nb = tf.square(a)\nc = tf.sqrt(a)\nd = b + c\ne = tf.matmul(a, b)\ne *= d\n\n\n\nA second look at the GradientTape API\nUsing the GradientTape\n\ninput_var = tf.Variable(initial_value=3.)\nwith tf.GradientTape() as tape:\n   result = tf.square(input_var)\ngradient = tape.gradient(result, input_var)\n\nUsing GradientTape with constant tensor inputs\n\ninput_const = tf.constant(3.)\nwith tf.GradientTape() as tape:\n   tape.watch(input_const)\n   result = tf.square(input_const)\ngradient = tape.gradient(result, input_const)\n\nUsing nested gradient tapes to compute second-order gradients\n\ntime = tf.Variable(0.)\nwith tf.GradientTape() as outer_tape:\n    with tf.GradientTape() as inner_tape:\n        position =  4.9 * time ** 2\n    speed = inner_tape.gradient(position, time)\nacceleration = outer_tape.gradient(speed, time)\n\n\n\nAn end-to-end example: A linear classifier in pure TensorFlow\nGenerating two classes of random points in a 2D plane\n\nnum_samples_per_class = 1000\nnegative_samples = np.random.multivariate_normal(\n    mean=[0, 3],\n    cov=[[1, 0.5],[0.5, 1]],\n    size=num_samples_per_class)\npositive_samples = np.random.multivariate_normal(\n    mean=[3, 0],\n    cov=[[1, 0.5],[0.5, 1]],\n    size=num_samples_per_class)\n\nStacking the two classes into an array with shape (2000, 2)\n\ninputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n\nGenerating the corresponding targets (0 and 1)\n\ntargets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=\"float32\"),\n                     np.ones((num_samples_per_class, 1), dtype=\"float32\")))\n\nPlotting the two point classes\n\nimport matplotlib.pyplot as plt\nplt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])\nplt.show()\n\nCreating the linear classifier variables\n\ninput_dim = 2\noutput_dim = 1\nW = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))\nb = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))\n\nThe forward pass function\n\ndef model(inputs):\n    return tf.matmul(inputs, W) + b\n\nThe mean squared error loss function\n\ndef square_loss(targets, predictions):\n    per_sample_losses = tf.square(targets - predictions)\n    return tf.reduce_mean(per_sample_losses)\n\nThe training step function\n\nlearning_rate = 0.1\n\ndef training_step(inputs, targets):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs)\n        loss = square_loss(predictions, targets)\n    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n    W.assign_sub(grad_loss_wrt_W * learning_rate)\n    b.assign_sub(grad_loss_wrt_b * learning_rate)\n    return loss\n\nThe batch training loop\n\nfor step in range(40):\n    loss = training_step(inputs, targets)\n    print(f\"Loss at step {step}: {loss:.4f}\")\n\n\npredictions = model(inputs)\nplt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)\nplt.show()\n\n\nx = np.linspace(-1, 4, 100)\ny = - W[0] /  W[1] * x + (0.5 - b) / W[1]\nplt.plot(x, y, \"-r\")\nplt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#anatomy-of-a-neural-network-understanding-core-keras-apis",
    "href": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#anatomy-of-a-neural-network-understanding-core-keras-apis",
    "title": "R Stats Bootcamp",
    "section": "Anatomy of a neural network: Understanding core Keras APIs",
    "text": "Anatomy of a neural network: Understanding core Keras APIs\n\nLayers: The building blocks of deep learning\n\nThe base Layer class in Keras\nA Dense layer implemented as a Layer subclass\n\nfrom tensorflow import keras\n\nclass SimpleDense(keras.layers.Layer):\n\n    def __init__(self, units, activation=None):\n        super().__init__()\n        self.units = units\n        self.activation = activation\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.W = self.add_weight(shape=(input_dim, self.units),\n                                 initializer=\"random_normal\")\n        self.b = self.add_weight(shape=(self.units,),\n                                 initializer=\"zeros\")\n\n    def call(self, inputs):\n        y = tf.matmul(inputs, self.W) + self.b\n        if self.activation is not None:\n            y = self.activation(y)\n        return y\n\n\nmy_dense = SimpleDense(units=32, activation=tf.nn.relu)\ninput_tensor = tf.ones(shape=(2, 784))\noutput_tensor = my_dense(input_tensor)\nprint(output_tensor.shape)\n\n\n\nAutomatic shape inference: Building layers on the fly\n\nfrom tensorflow.keras import layers\nlayer = layers.Dense(32, activation=\"relu\")\n\n\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nmodel = models.Sequential([\n    layers.Dense(32, activation=\"relu\"),\n    layers.Dense(32)\n])\n\n\nmodel = keras.Sequential([\n    SimpleDense(32, activation=\"relu\"),\n    SimpleDense(64, activation=\"relu\"),\n    SimpleDense(32, activation=\"relu\"),\n    SimpleDense(10, activation=\"softmax\")\n])\n\n\n\n\nFrom layers to models\n\n\nThe “compile” step: Configuring the learning process\n\nmodel = keras.Sequential([keras.layers.Dense(1)])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"mean_squared_error\",\n              metrics=[\"accuracy\"])\n\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(),\n              loss=keras.losses.MeanSquaredError(),\n              metrics=[keras.metrics.BinaryAccuracy()])\n\n\n\nPicking a loss function\n\n\nUnderstanding the fit() method\nCalling fit() with NumPy data\n\nhistory = model.fit(\n    inputs,\n    targets,\n    epochs=5,\n    batch_size=128\n)\n\n\nhistory.history\n\n\n\nMonitoring loss and metrics on validation data\nUsing the validation_data argument\n\nmodel = keras.Sequential([keras.layers.Dense(1)])\nmodel.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n              loss=keras.losses.MeanSquaredError(),\n              metrics=[keras.metrics.BinaryAccuracy()])\n\nindices_permutation = np.random.permutation(len(inputs))\nshuffled_inputs = inputs[indices_permutation]\nshuffled_targets = targets[indices_permutation]\n\nnum_validation_samples = int(0.3 * len(inputs))\nval_inputs = shuffled_inputs[:num_validation_samples]\nval_targets = shuffled_targets[:num_validation_samples]\ntraining_inputs = shuffled_inputs[num_validation_samples:]\ntraining_targets = shuffled_targets[num_validation_samples:]\nmodel.fit(\n    training_inputs,\n    training_targets,\n    epochs=5,\n    batch_size=16,\n    validation_data=(val_inputs, val_targets)\n)\n\n\n\nInference: Using a model after training\n\npredictions = model.predict(val_inputs, batch_size=128)\nprint(predictions[:10])"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#summary",
    "href": "ipy-notebooks/Chollet-chapter03_introduction-to-keras-and-tf.html#summary",
    "title": "R Stats Bootcamp",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html",
    "href": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html",
    "title": "R Stats Bootcamp",
    "section": "",
    "text": "This is a companion notebook for the book Deep Learning with Python, Second Edition. For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\nIf you want to be able to follow what’s going on, I recommend reading the notebook side by side with your copy of the book.\nThis notebook was generated for TensorFlow 2.6."
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html#classifying-movie-reviews-a-binary-classification-example",
    "href": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html#classifying-movie-reviews-a-binary-classification-example",
    "title": "R Stats Bootcamp",
    "section": "Classifying movie reviews: A binary classification example",
    "text": "Classifying movie reviews: A binary classification example\n\nThe IMDB dataset\nLoading the IMDB dataset\n\nfrom tensorflow.keras.datasets import imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=10000)\n\n\ntrain_data[0]\n\n\ntrain_labels[0]\n\n\nmax([max(sequence) for sequence in train_data])\n\nDecoding reviews back to text\n\nword_index = imdb.get_word_index()\nreverse_word_index = dict(\n    [(value, key) for (key, value) in word_index.items()])\ndecoded_review = \" \".join(\n    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n\n\n\nPreparing the data\nEncoding the integer sequences via multi-hot encoding\n\nimport numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        for j in sequence:\n            results[i, j] = 1.\n    return results\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\n\nx_train[0]\n\n\ny_train = np.asarray(train_labels).astype(\"float32\")\ny_test = np.asarray(test_labels).astype(\"float32\")\n\n\n\nBuilding your model\nModel definition\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nCompiling the model\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\n\n\n\nValidating your approach\nSetting aside a validation set\n\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n\nTraining your model\n\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n\n\nhistory_dict = history.history\nhistory_dict.keys()\n\nPlotting the training and validation loss\n\nimport matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\nPlotting the training and validation accuracy\n\nplt.clf()\nacc = history_dict[\"accuracy\"]\nval_acc = history_dict[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training acc\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\nRetraining a model from scratch\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nresults = model.evaluate(x_test, y_test)\n\n\nresults\n\n\n\nUsing a trained model to generate predictions on new data\n\nmodel.predict(x_test)\n\n\n\nFurther experiments\n\n\nWrapping up"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html#classifying-newswires-a-multiclass-classification-example",
    "href": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html#classifying-newswires-a-multiclass-classification-example",
    "title": "R Stats Bootcamp",
    "section": "Classifying newswires: A multiclass classification example",
    "text": "Classifying newswires: A multiclass classification example\n\nThe Reuters dataset\nLoading the Reuters dataset\n\nfrom tensorflow.keras.datasets import reuters\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n    num_words=10000)\n\n\nlen(train_data)\n\n\nlen(test_data)\n\n\ntrain_data[10]\n\nDecoding newswires back to text\n\nword_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in\n    train_data[0]])\n\n\ntrain_labels[10]\n\n\n\nPreparing the data\nEncoding the input data\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\nEncoding the labels\n\ndef to_one_hot(labels, dimension=46):\n    results = np.zeros((len(labels), dimension))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.\n    return results\ny_train = to_one_hot(train_labels)\ny_test = to_one_hot(test_labels)\n\n\nfrom tensorflow.keras.utils import to_categorical\ny_train = to_categorical(train_labels)\ny_test = to_categorical(test_labels)\n\n\n\nBuilding your model\nModel definition\n\nmodel = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(46, activation=\"softmax\")\n])\n\nCompiling the model\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n\n\nValidating your approach\nSetting aside a validation set\n\nx_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = y_train[:1000]\npartial_y_train = y_train[1000:]\n\nTraining the model\n\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n\nPlotting the training and validation loss\n\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\nPlotting the training and validation accuracy\n\nplt.clf()\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\nRetraining a model from scratch\n\nmodel = keras.Sequential([\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(46, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train,\n          y_train,\n          epochs=9,\n          batch_size=512)\nresults = model.evaluate(x_test, y_test)\n\n\nresults\n\n\nimport copy\ntest_labels_copy = copy.copy(test_labels)\nnp.random.shuffle(test_labels_copy)\nhits_array = np.array(test_labels) == np.array(test_labels_copy)\nhits_array.mean()\n\n\n\nGenerating predictions on new data\n\npredictions = model.predict(x_test)\n\n\npredictions[0].shape\n\n\nnp.sum(predictions[0])\n\n\nnp.argmax(predictions[0])\n\n\n\nA different way to handle the labels and the loss\n\ny_train = np.array(train_labels)\ny_test = np.array(test_labels)\n\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n\n\nThe importance of having sufficiently large intermediate layers\nA model with an information bottleneck\n\nmodel = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(4, activation=\"relu\"),\n    layers.Dense(46, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(partial_x_train,\n          partial_y_train,\n          epochs=20,\n          batch_size=128,\n          validation_data=(x_val, y_val))\n\n\n\nFurther experiments\n\n\nWrapping up"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html#predicting-house-prices-a-regression-example",
    "href": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html#predicting-house-prices-a-regression-example",
    "title": "R Stats Bootcamp",
    "section": "Predicting house prices: A regression example",
    "text": "Predicting house prices: A regression example\n\nThe Boston Housing Price dataset\nLoading the Boston housing dataset\n\nfrom tensorflow.keras.datasets import boston_housing\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n\n\ntrain_data.shape\n\n\ntest_data.shape\n\n\ntrain_targets\n\n\n\nPreparing the data\nNormalizing the data\n\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\ntest_data -= mean\ntest_data /= std\n\n\n\nBuilding your model\nModel definition\n\ndef build_model():\n    model = keras.Sequential([\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(1)\n    ])\n    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n    return model\n\n\n\nValidating your approach using K-fold validation\nK-fold validation\n\nk = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print(f\"Processing fold #{i}\")\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    model.fit(partial_train_data, partial_train_targets,\n              epochs=num_epochs, batch_size=16, verbose=0)\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)\n\n\nall_scores\n\n\nnp.mean(all_scores)\n\nSaving the validation logs at each fold\n\nnum_epochs = 500\nall_mae_histories = []\nfor i in range(k):\n    print(f\"Processing fold #{i}\")\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    history = model.fit(partial_train_data, partial_train_targets,\n                        validation_data=(val_data, val_targets),\n                        epochs=num_epochs, batch_size=16, verbose=0)\n    mae_history = history.history[\"val_mae\"]\n    all_mae_histories.append(mae_history)\n\nBuilding the history of successive mean K-fold validation scores\n\naverage_mae_history = [\n    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n\nPlotting validation scores\n\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()\n\nPlotting validation scores, excluding the first 10 data points\n\ntruncated_mae_history = average_mae_history[10:]\nplt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()\n\nTraining the final model\n\nmodel = build_model()\nmodel.fit(train_data, train_targets,\n          epochs=130, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n\n\ntest_mae_score\n\n\n\nGenerating predictions on new data\n\npredictions = model.predict(test_data)\npredictions[0]\n\n\n\nWrapping up"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html#summary",
    "href": "ipy-notebooks/Chollet-chapter04_getting-started-with-neural-networks.html#summary",
    "title": "R Stats Bootcamp",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html",
    "href": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html",
    "title": "R Stats Bootcamp",
    "section": "",
    "text": "This is a companion notebook for the book Deep Learning with Python, Second Edition. For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\nIf you want to be able to follow what’s going on, I recommend reading the notebook side by side with your copy of the book.\nThis notebook was generated for TensorFlow 2.6."
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#generalization-the-goal-of-machine-learning",
    "href": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#generalization-the-goal-of-machine-learning",
    "title": "R Stats Bootcamp",
    "section": "Generalization: The goal of machine learning",
    "text": "Generalization: The goal of machine learning\n\nUnderfitting and overfitting\n\nNoisy training data\n\n\nAmbiguous features\n\n\nRare features and spurious correlations\nAdding white-noise channels or all-zeros channels to MNIST\n\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np\n\n(train_images, train_labels), _ = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\n\ntrain_images_with_noise_channels = np.concatenate(\n    [train_images, np.random.random((len(train_images), 784))], axis=1)\n\ntrain_images_with_zeros_channels = np.concatenate(\n    [train_images, np.zeros((len(train_images), 784))], axis=1)\n\nTraining the same model on MNIST data with noise channels or all-zero channels\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef get_model():\n    model = keras.Sequential([\n        layers.Dense(512, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\")\n    ])\n    model.compile(optimizer=\"rmsprop\",\n                  loss=\"sparse_categorical_crossentropy\",\n                  metrics=[\"accuracy\"])\n    return model\n\nmodel = get_model()\nhistory_noise = model.fit(\n    train_images_with_noise_channels, train_labels,\n    epochs=10,\n    batch_size=128,\n    validation_split=0.2)\n\nmodel = get_model()\nhistory_zeros = model.fit(\n    train_images_with_zeros_channels, train_labels,\n    epochs=10,\n    batch_size=128,\n    validation_split=0.2)\n\nPlotting a validation accuracy comparison\n\nimport matplotlib.pyplot as plt\nval_acc_noise = history_noise.history[\"val_accuracy\"]\nval_acc_zeros = history_zeros.history[\"val_accuracy\"]\nepochs = range(1, 11)\nplt.plot(epochs, val_acc_noise, \"b-\",\n         label=\"Validation accuracy with noise channels\")\nplt.plot(epochs, val_acc_zeros, \"b--\",\n         label=\"Validation accuracy with zeros channels\")\nplt.title(\"Effect of noise channels on validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\n\n\n\nThe nature of generalization in deep learning\nFitting a MNIST model with randomly shuffled labels\n\n(train_images, train_labels), _ = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\n\nrandom_train_labels = train_labels[:]\nnp.random.shuffle(random_train_labels)\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation=\"relu\"),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_images, random_train_labels,\n          epochs=100,\n          batch_size=128,\n          validation_split=0.2)\n\n\nThe manifold hypothesis\n\n\nInterpolation as a source of generalization\n\n\nWhy deep learning works\n\n\nTraining data is paramount"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#evaluating-machine-learning-models",
    "href": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#evaluating-machine-learning-models",
    "title": "R Stats Bootcamp",
    "section": "Evaluating machine-learning models",
    "text": "Evaluating machine-learning models\n\nTraining, validation, and test sets\n\nSimple hold-out validation\n\n\nK-fold validation\n\n\nIterated K-fold validation with shuffling\n\n\n\nBeating a common-sense baseline\n\n\nThings to keep in mind about model evaluation"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#improving-model-fit",
    "href": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#improving-model-fit",
    "title": "R Stats Bootcamp",
    "section": "Improving model fit",
    "text": "Improving model fit\n\nTuning key gradient descent parameters\nTraining a MNIST model with an incorrectly high learning rate\n\n(train_images, train_labels), _ = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype(\"float32\") / 255\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation=\"relu\"),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=keras.optimizers.RMSprop(1.),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_images, train_labels,\n          epochs=10,\n          batch_size=128,\n          validation_split=0.2)\n\nThe same model with a more appropriate learning rate\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation=\"relu\"),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=keras.optimizers.RMSprop(1e-2),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_images, train_labels,\n          epochs=10,\n          batch_size=128,\n          validation_split=0.2)\n\n\n\nLeveraging better architecture priors\n\n\nIncreasing model capacity\nA simple logistic regression on MNIST\n\nmodel = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nhistory_small_model = model.fit(\n    train_images, train_labels,\n    epochs=20,\n    batch_size=128,\n    validation_split=0.2)\n\n\nimport matplotlib.pyplot as plt\nval_loss = history_small_model.history[\"val_loss\"]\nepochs = range(1, 21)\nplt.plot(epochs, val_loss, \"b--\",\n         label=\"Validation loss\")\nplt.title(\"Effect of insufficient model capacity on validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\nmodel = keras.Sequential([\n    layers.Dense(96, activation=\"relu\"),\n    layers.Dense(96, activation=\"relu\"),\n    layers.Dense(10, activation=\"softmax\"),\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nhistory_large_model = model.fit(\n    train_images, train_labels,\n    epochs=20,\n    batch_size=128,\n    validation_split=0.2)"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#improving-generalization",
    "href": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#improving-generalization",
    "title": "R Stats Bootcamp",
    "section": "Improving generalization",
    "text": "Improving generalization\n\nDataset curation\n\n\nFeature engineering\n\n\nUsing early stopping\n\n\nRegularizing your model\n\nReducing the network’s size\nOriginal model\n\nfrom tensorflow.keras.datasets import imdb\n(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\ntrain_data = vectorize_sequences(train_data)\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nhistory_original = model.fit(train_data, train_labels,\n                             epochs=20, batch_size=512, validation_split=0.4)\n\nVersion of the model with lower capacity\n\nmodel = keras.Sequential([\n    layers.Dense(4, activation=\"relu\"),\n    layers.Dense(4, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nhistory_smaller_model = model.fit(\n    train_data, train_labels,\n    epochs=20, batch_size=512, validation_split=0.4)\n\nVersion of the model with higher capacity\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation=\"relu\"),\n    layers.Dense(512, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nhistory_larger_model = model.fit(\n    train_data, train_labels,\n    epochs=20, batch_size=512, validation_split=0.4)\n\n\n\nAdding weight regularization\nAdding L2 weight regularization to the model\n\nfrom tensorflow.keras import regularizers\nmodel = keras.Sequential([\n    layers.Dense(16,\n                 kernel_regularizer=regularizers.l2(0.002),\n                 activation=\"relu\"),\n    layers.Dense(16,\n                 kernel_regularizer=regularizers.l2(0.002),\n                 activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nhistory_l2_reg = model.fit(\n    train_data, train_labels,\n    epochs=20, batch_size=512, validation_split=0.4)\n\nDifferent weight regularizers available in Keras\n\nfrom tensorflow.keras import regularizers\nregularizers.l1(0.001)\nregularizers.l1_l2(l1=0.001, l2=0.001)\n\n\n\nAdding dropout\nAdding dropout to the IMDB model\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dropout(0.5),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nhistory_dropout = model.fit(\n    train_data, train_labels,\n    epochs=20, batch_size=512, validation_split=0.4)"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#summary",
    "href": "ipy-notebooks/Chollet-chapter05_fundamentals-of-ml.html#summary",
    "title": "R Stats Bootcamp",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter07_working-with-keras.html",
    "href": "ipy-notebooks/Chollet-chapter07_working-with-keras.html",
    "title": "R Stats Bootcamp",
    "section": "",
    "text": "This is a companion notebook for the book Deep Learning with Python, Second Edition. For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\nIf you want to be able to follow what’s going on, I recommend reading the notebook side by side with your copy of the book.\nThis notebook was generated for TensorFlow 2.6."
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#a-spectrum-of-workflows",
    "href": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#a-spectrum-of-workflows",
    "title": "R Stats Bootcamp",
    "section": "A spectrum of workflows",
    "text": "A spectrum of workflows"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#different-ways-to-build-keras-models",
    "href": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#different-ways-to-build-keras-models",
    "title": "R Stats Bootcamp",
    "section": "Different ways to build Keras models",
    "text": "Different ways to build Keras models\n\nThe Sequential model\nThe Sequential class\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(10, activation=\"softmax\")\n])\n\nIncrementally building a Sequential model\n\nmodel = keras.Sequential()\nmodel.add(layers.Dense(64, activation=\"relu\"))\nmodel.add(layers.Dense(10, activation=\"softmax\"))\n\nCalling a model for the first time to build it\n\nmodel.build(input_shape=(None, 3))\nmodel.weights\n\nThe summary method\n\nmodel.summary()\n\nNaming models and layers with the name argument\n\nmodel = keras.Sequential(name=\"my_example_model\")\nmodel.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\nmodel.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))\nmodel.build((None, 3))\nmodel.summary()\n\nSpecifying the input shape of your model in advance\n\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(3,)))\nmodel.add(layers.Dense(64, activation=\"relu\"))\n\n\nmodel.summary()\n\n\nmodel.add(layers.Dense(10, activation=\"softmax\"))\nmodel.summary()\n\n\n\nThe Functional API\n\nA simple example\nA simple Functional model with two Dense layers\n\ninputs = keras.Input(shape=(3,), name=\"my_input\")\nfeatures = layers.Dense(64, activation=\"relu\")(inputs)\noutputs = layers.Dense(10, activation=\"softmax\")(features)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n\ninputs = keras.Input(shape=(3,), name=\"my_input\")\n\n\ninputs.shape\n\n\ninputs.dtype\n\n\nfeatures = layers.Dense(64, activation=\"relu\")(inputs)\n\n\nfeatures.shape\n\n\noutputs = layers.Dense(10, activation=\"softmax\")(features)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n\nmodel.summary()\n\n\n\nMulti-input, multi-output models\nA multi-input, multi-output Functional model\n\nvocabulary_size = 10000\nnum_tags = 100\nnum_departments = 4\n\ntitle = keras.Input(shape=(vocabulary_size,), name=\"title\")\ntext_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\ntags = keras.Input(shape=(num_tags,), name=\"tags\")\n\nfeatures = layers.Concatenate()([title, text_body, tags])\nfeatures = layers.Dense(64, activation=\"relu\")(features)\n\npriority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\ndepartment = layers.Dense(\n    num_departments, activation=\"softmax\", name=\"department\")(features)\n\nmodel = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])\n\n\n\nTraining a multi-input, multi-output model\nTraining a model by providing lists of input & target arrays\n\nimport numpy as np\n\nnum_samples = 1280\n\ntitle_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\ntext_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\ntags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n\npriority_data = np.random.random(size=(num_samples, 1))\ndepartment_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\nmodel.fit([title_data, text_body_data, tags_data],\n          [priority_data, department_data],\n          epochs=1)\nmodel.evaluate([title_data, text_body_data, tags_data],\n               [priority_data, department_data])\npriority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])\n\nTraining a model by providing dicts of input & target arrays\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"},\n              metrics={\"priority\": [\"mean_absolute_error\"], \"department\": [\"accuracy\"]})\nmodel.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n          {\"priority\": priority_data, \"department\": department_data},\n          epochs=1)\nmodel.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n               {\"priority\": priority_data, \"department\": department_data})\npriority_preds, department_preds = model.predict(\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})\n\n\n\nThe power of the Functional API: Access to layer connectivity\n\nkeras.utils.plot_model(model, \"ticket_classifier.png\")\n\n\nkeras.utils.plot_model(model, \"ticket_classifier_with_shape_info.png\", show_shapes=True)\n\nRetrieving the inputs or outputs of a layer in a Functional model\n\nmodel.layers\n\n\nmodel.layers[3].input\n\n\nmodel.layers[3].output\n\nCreating a new model by reusing intermediate layer outputs\n\nfeatures = model.layers[4].output\ndifficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n\nnew_model = keras.Model(\n    inputs=[title, text_body, tags],\n    outputs=[priority, department, difficulty])\n\n\nkeras.utils.plot_model(new_model, \"updated_ticket_classifier.png\", show_shapes=True)\n\n\n\n\nSubclassing the Model class\n\nRewriting our previous example as a subclassed model\nA simple subclassed model\n\nclass CustomerTicketModel(keras.Model):\n\n    def __init__(self, num_departments):\n        super().__init__()\n        self.concat_layer = layers.Concatenate()\n        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n        self.department_classifier = layers.Dense(\n            num_departments, activation=\"softmax\")\n\n    def call(self, inputs):\n        title = inputs[\"title\"]\n        text_body = inputs[\"text_body\"]\n        tags = inputs[\"tags\"]\n\n        features = self.concat_layer([title, text_body, tags])\n        features = self.mixing_layer(features)\n        priority = self.priority_scorer(features)\n        department = self.department_classifier(features)\n        return priority, department\n\n\nmodel = CustomerTicketModel(num_departments=4)\n\npriority, department = model(\n    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})\n\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\nmodel.fit({\"title\": title_data,\n           \"text_body\": text_body_data,\n           \"tags\": tags_data},\n          [priority_data, department_data],\n          epochs=1)\nmodel.evaluate({\"title\": title_data,\n                \"text_body\": text_body_data,\n                \"tags\": tags_data},\n               [priority_data, department_data])\npriority_preds, department_preds = model.predict({\"title\": title_data,\n                                                  \"text_body\": text_body_data,\n                                                  \"tags\": tags_data})\n\n\n\nBeware: What subclassed models don’t support\n\n\n\nMixing and matching different components\nCreating a Functional model that includes a subclassed model\n\nclass Classifier(keras.Model):\n\n    def __init__(self, num_classes=2):\n        super().__init__()\n        if num_classes == 2:\n            num_units = 1\n            activation = \"sigmoid\"\n        else:\n            num_units = num_classes\n            activation = \"softmax\"\n        self.dense = layers.Dense(num_units, activation=activation)\n\n    def call(self, inputs):\n        return self.dense(inputs)\n\ninputs = keras.Input(shape=(3,))\nfeatures = layers.Dense(64, activation=\"relu\")(inputs)\noutputs = Classifier(num_classes=10)(features)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\nCreating a subclassed model that includes a Functional model\n\ninputs = keras.Input(shape=(64,))\noutputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\nbinary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n\nclass MyModel(keras.Model):\n\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.dense = layers.Dense(64, activation=\"relu\")\n        self.classifier = binary_classifier\n\n    def call(self, inputs):\n        features = self.dense(inputs)\n        return self.classifier(features)\n\nmodel = MyModel()\n\n\n\nRemember: Use the right tool for the job"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#using-built-in-training-and-evaluation-loops",
    "href": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#using-built-in-training-and-evaluation-loops",
    "title": "R Stats Bootcamp",
    "section": "Using built-in training and evaluation loops",
    "text": "Using built-in training and evaluation loops\nThe standard workflow: compile(), fit(), evaluate(), predict()\n\nfrom tensorflow.keras.datasets import mnist\n\ndef get_mnist_model():\n    inputs = keras.Input(shape=(28 * 28,))\n    features = layers.Dense(512, activation=\"relu\")(inputs)\n    features = layers.Dropout(0.5)(features)\n    outputs = layers.Dense(10, activation=\"softmax\")(features)\n    model = keras.Model(inputs, outputs)\n    return model\n\n(images, labels), (test_images, test_labels) = mnist.load_data()\nimages = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\ntest_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\ntrain_images, val_images = images[10000:], images[:10000]\ntrain_labels, val_labels = labels[10000:], labels[:10000]\n\nmodel = get_mnist_model()\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_images, train_labels,\n          epochs=3,\n          validation_data=(val_images, val_labels))\ntest_metrics = model.evaluate(test_images, test_labels)\npredictions = model.predict(test_images)\n\n\nWriting your own metrics\nImplementing a custom metric by subclassing the Metric class\n\nimport tensorflow as tf\n\nclass RootMeanSquaredError(keras.metrics.Metric):\n\n    def __init__(self, name=\"rmse\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n        self.total_samples = self.add_weight(\n            name=\"total_samples\", initializer=\"zeros\", dtype=\"int32\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])\n        mse = tf.reduce_sum(tf.square(y_true - y_pred))\n        self.mse_sum.assign_add(mse)\n        num_samples = tf.shape(y_pred)[0]\n        self.total_samples.assign_add(num_samples)\n\n    def result(self):\n        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))\n\n    def reset_state(self):\n        self.mse_sum.assign(0.)\n        self.total_samples.assign(0)\n\n\nmodel = get_mnist_model()\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\", RootMeanSquaredError()])\nmodel.fit(train_images, train_labels,\n          epochs=3,\n          validation_data=(val_images, val_labels))\ntest_metrics = model.evaluate(test_images, test_labels)\n\n\n\nUsing callbacks\n\nThe EarlyStopping and ModelCheckpoint callbacks\nUsing the callbacks argument in the fit() method\n\ncallbacks_list = [\n    keras.callbacks.EarlyStopping(\n        monitor=\"val_accuracy\",\n        patience=2,\n    ),\n    keras.callbacks.ModelCheckpoint(\n        filepath=\"checkpoint_path.keras\",\n        monitor=\"val_loss\",\n        save_best_only=True,\n    )\n]\nmodel = get_mnist_model()\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_images, train_labels,\n          epochs=10,\n          callbacks=callbacks_list,\n          validation_data=(val_images, val_labels))\n\n\nmodel = keras.models.load_model(\"checkpoint_path.keras\")\n\n\n\n\nWriting your own callbacks\nCreating a custom callback by subclassing the Callback class\n\nfrom matplotlib import pyplot as plt\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs):\n        self.per_batch_losses = []\n\n    def on_batch_end(self, batch, logs):\n        self.per_batch_losses.append(logs.get(\"loss\"))\n\n    def on_epoch_end(self, epoch, logs):\n        plt.clf()\n        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,\n                 label=\"Training loss for each batch\")\n        plt.xlabel(f\"Batch (epoch {epoch})\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n        plt.savefig(f\"plot_at_epoch_{epoch}\")\n        self.per_batch_losses = []\n\n\nmodel = get_mnist_model()\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_images, train_labels,\n          epochs=10,\n          callbacks=[LossHistory()],\n          validation_data=(val_images, val_labels))\n\n\n\nMonitoring and visualization with TensorBoard\n\nmodel = get_mnist_model()\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\ntensorboard = keras.callbacks.TensorBoard(\n    log_dir=\"/full_path_to_your_log_dir\",\n)\nmodel.fit(train_images, train_labels,\n          epochs=10,\n          validation_data=(val_images, val_labels),\n          callbacks=[tensorboard])\n\n\n%load_ext tensorboard\n%tensorboard --logdir /full_path_to_your_log_dir"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#writing-your-own-training-and-evaluation-loops",
    "href": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#writing-your-own-training-and-evaluation-loops",
    "title": "R Stats Bootcamp",
    "section": "Writing your own training and evaluation loops",
    "text": "Writing your own training and evaluation loops\n\nTraining versus inference\n\n\nLow-level usage of metrics\n\nmetric = keras.metrics.SparseCategoricalAccuracy()\ntargets = [0, 1, 2]\npredictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nmetric.update_state(targets, predictions)\ncurrent_result = metric.result()\nprint(f\"result: {current_result:.2f}\")\n\n\nvalues = [0, 1, 2, 3, 4]\nmean_tracker = keras.metrics.Mean()\nfor value in values:\n    mean_tracker.update_state(value)\nprint(f\"Mean of values: {mean_tracker.result():.2f}\")\n\n\n\nA complete training and evaluation loop\nWriting a step-by-step training loop: the training step function\n\nmodel = get_mnist_model()\n\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\noptimizer = keras.optimizers.RMSprop()\nmetrics = [keras.metrics.SparseCategoricalAccuracy()]\nloss_tracking_metric = keras.metrics.Mean()\n\ndef train_step(inputs, targets):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs, training=True)\n        loss = loss_fn(targets, predictions)\n    gradients = tape.gradient(loss, model.trainable_weights)\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n\n    logs = {}\n    for metric in metrics:\n        metric.update_state(targets, predictions)\n        logs[metric.name] = metric.result()\n\n    loss_tracking_metric.update_state(loss)\n    logs[\"loss\"] = loss_tracking_metric.result()\n    return logs\n\nWriting a step-by-step training loop: resetting the metrics\n\ndef reset_metrics():\n    for metric in metrics:\n        metric.reset_state()\n    loss_tracking_metric.reset_state()\n\nWriting a step-by-step training loop: the loop itself\n\ntraining_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\ntraining_dataset = training_dataset.batch(32)\nepochs = 3\nfor epoch in range(epochs):\n    reset_metrics()\n    for inputs_batch, targets_batch in training_dataset:\n        logs = train_step(inputs_batch, targets_batch)\n    print(f\"Results at the end of epoch {epoch}\")\n    for key, value in logs.items():\n        print(f\"...{key}: {value:.4f}\")\n\nWriting a step-by-step evaluation loop\n\ndef test_step(inputs, targets):\n    predictions = model(inputs, training=False)\n    loss = loss_fn(targets, predictions)\n\n    logs = {}\n    for metric in metrics:\n        metric.update_state(targets, predictions)\n        logs[\"val_\" + metric.name] = metric.result()\n\n    loss_tracking_metric.update_state(loss)\n    logs[\"val_loss\"] = loss_tracking_metric.result()\n    return logs\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\nval_dataset = val_dataset.batch(32)\nreset_metrics()\nfor inputs_batch, targets_batch in val_dataset:\n    logs = test_step(inputs_batch, targets_batch)\nprint(\"Evaluation results:\")\nfor key, value in logs.items():\n    print(f\"...{key}: {value:.4f}\")\n\n\n\nMake it fast with tf.function\nAdding a tf.function decorator to our evaluation-step function\n\n@tf.function\ndef test_step(inputs, targets):\n    predictions = model(inputs, training=False)\n    loss = loss_fn(targets, predictions)\n\n    logs = {}\n    for metric in metrics:\n        metric.update_state(targets, predictions)\n        logs[\"val_\" + metric.name] = metric.result()\n\n    loss_tracking_metric.update_state(loss)\n    logs[\"val_loss\"] = loss_tracking_metric.result()\n    return logs\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\nval_dataset = val_dataset.batch(32)\nreset_metrics()\nfor inputs_batch, targets_batch in val_dataset:\n    logs = test_step(inputs_batch, targets_batch)\nprint(\"Evaluation results:\")\nfor key, value in logs.items():\n    print(f\"...{key}: {value:.4f}\")\n\n\n\nLeveraging fit() with a custom training loop\nImplementing a custom training step to use with fit()\n\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\nloss_tracker = keras.metrics.Mean(name=\"loss\")\n\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        inputs, targets = data\n        with tf.GradientTape() as tape:\n            predictions = self(inputs, training=True)\n            loss = loss_fn(targets, predictions)\n        gradients = tape.gradient(loss, model.trainable_weights)\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n\n        loss_tracker.update_state(loss)\n        return {\"loss\": loss_tracker.result()}\n\n    @property\n    def metrics(self):\n        return [loss_tracker]\n\n\ninputs = keras.Input(shape=(28 * 28,))\nfeatures = layers.Dense(512, activation=\"relu\")(inputs)\nfeatures = layers.Dropout(0.5)(features)\noutputs = layers.Dense(10, activation=\"softmax\")(features)\nmodel = CustomModel(inputs, outputs)\n\nmodel.compile(optimizer=keras.optimizers.RMSprop())\nmodel.fit(train_images, train_labels, epochs=3)\n\n\nclass CustomModel(keras.Model):\n    def train_step(self, data):\n        inputs, targets = data\n        with tf.GradientTape() as tape:\n            predictions = self(inputs, training=True)\n            loss = self.compiled_loss(targets, predictions)\n        gradients = tape.gradient(loss, model.trainable_weights)\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n        self.compiled_metrics.update_state(targets, predictions)\n        return {m.name: m.result() for m in self.metrics}\n\n\ninputs = keras.Input(shape=(28 * 28,))\nfeatures = layers.Dense(512, activation=\"relu\")(inputs)\nfeatures = layers.Dropout(0.5)(features)\noutputs = layers.Dense(10, activation=\"softmax\")(features)\nmodel = CustomModel(inputs, outputs)\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(),\n              loss=keras.losses.SparseCategoricalCrossentropy(),\n              metrics=[keras.metrics.SparseCategoricalAccuracy()])\nmodel.fit(train_images, train_labels, epochs=3)"
  },
  {
    "objectID": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#summary",
    "href": "ipy-notebooks/Chollet-chapter07_working-with-keras.html#summary",
    "title": "R Stats Bootcamp",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The material in this module is designed to be experienced in an intensive one week format followed by an assessment meant to showcase reproducible statistical analysis skills. For enrolled students, the work will be supported with several live sessions during the main week of delivery.\n\nPreparation: If you have no Python programming experience or would like to strengthen your skills, you may find it useful to practice before the module week.\n\n(thorough) Official Python Tutorial\n(full course) Dr. Chuck on FreeCodeCamp\n\n\n\n\n\nDay\nTopics\nLabs\n\n\n\n\nMon\nam\n vid1\n vid2\npm\n vid1\n 01 Introduction  1.1\n 02 Linear algebra  2.1\n 03 Probability  3.1\n00 Lab Preparation\n01 Anaconda and Python labs\n  01 Python labs 1:7\n  02 Pandas labs 1:6\n\n\nTues\nam\n vid1\npm\n(no vid)\n 04 Numerical computation  4.1  4.2\n 05 Machine learning  5.1\n 06 Deep networks  6.1  6.2\n Chollet 2021 Ch 03\n02 Colab deep learning labs\n 03 Deep learning labs 1:6\n 04 Computer viz labs 1:6\n\n\nWed\nam\n(no meeting)\npm\n(no meeting)\n 07 Regularization  7.1\n 08 Stochastic gradient descent  8.1  8.2\nFinish previous or work through\n Chollet 2021\nCh 04, Ch 05, Ch06,\nCh 07, Ch 08, Ch 09\n Ch 04 notebook\n Ch 05 notebook\n Ch 07 notebook\n Ch 08 notebook\nCh  9.1,  9.2,  9.3\n\n\nThurs\nam\npm\n 09 Convolutional networks  9.1  9.2\n 10 Recurrent networks  10.1\nTensorflow example + your own experiments\n weevil watch repo\n\n\nFri\nam\npm\n 11 Practical methods  11.1\n 12 Applications  12.1  12.2\nYolo example + your own experiments"
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  }
]